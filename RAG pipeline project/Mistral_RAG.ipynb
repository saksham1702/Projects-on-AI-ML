{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:03:15.173893Z",
     "start_time": "2025-01-30T14:03:15.169039Z"
    }
   },
   "cell_type": "code",
   "source": "directory_path = \"/Mistral_/Data_Abstract\"",
   "id": "457f8db3ba83389d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T14:03:22.742616Z",
     "start_time": "2025-01-30T14:03:15.196596Z"
    }
   },
   "source": [
    "import os\n",
    "import uuid\n",
    "import re\n",
    "import json\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    base = os.path.basename(file_path)\n",
    "    sku = os.path.splitext(base)[0]\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pookie/PycharmProjects/PythonProject1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:03:23.630617Z",
     "start_time": "2025-01-30T14:03:23.627792Z"
    }
   },
   "cell_type": "code",
   "source": "doc_id = str(uuid.uuid4())\n",
   "id": "1502da91c2dd66fc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:03:23.829701Z",
     "start_time": "2025-01-30T14:03:23.822215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunking(directory_path, model_name, chunk_size,para_seperator=\"/n/n\", separator=\" \"):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    documents = {}\n",
    "    all_chunks = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        base = os.path.basename(file_path)\n",
    "        sku = os.path.splitext(base)[0]\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            doc_id = str(uuid.uuid4())\n",
    "\n",
    "            paragraphs = re.split(para_seperator, text)\n",
    "\n",
    "            for paragraph in paragraphs:\n",
    "                words = paragraph.split(separator)\n",
    "                current_chunk_str = \"\"\n",
    "                chunk = []\n",
    "                for word in words:\n",
    "                    if current_chunk_str:\n",
    "                        new_chunk = current_chunk_str + separator + word\n",
    "                    else:\n",
    "                        new_chunk = current_chunk_str + word\n",
    "                    if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
    "                        current_chunk_str = new_chunk\n",
    "                    else:\n",
    "                        if current_chunk_str:\n",
    "                            chunk.append(current_chunk_str)\n",
    "                        current_chunk_str = word\n",
    "\n",
    "                if current_chunk_str:\n",
    "                    chunk.append(current_chunk_str)\n",
    "\n",
    "                for chunk in chunk:\n",
    "                    chunk_id = str(uuid.uuid4())\n",
    "                    all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"file_name\":sku}}\n",
    "        documents[doc_id] = all_chunks\n",
    "    return documents"
   ],
   "id": "57476fcfabfac1c6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:03:44.542699Z",
     "start_time": "2025-01-30T14:03:24.029082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def map_document_embeddings(documents, tokenizer, model):\n",
    "    mapped_document_db = {}\n",
    "    for id, dict_content in documents.items():\n",
    "        mapped_embeddings = {}\n",
    "        for content_id, text_content in dict_content.items():\n",
    "            text = text_content.get(\"text\")\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "            mapped_embeddings[content_id] = embeddings\n",
    "        mapped_document_db[id] = mapped_embeddings\n",
    "    return mapped_document_db"
   ],
   "id": "2f9a445e454eb2e1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:03:44.549203Z",
     "start_time": "2025-01-30T14:03:44.547034Z"
    }
   },
   "cell_type": "code",
   "source": "path = \" /Users/pookie/PycharmProjects/PythonProject1/.venv/Mistral_/Data.json \"",
   "id": "df0e9a1ace50c379",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:03:44.562185Z",
     "start_time": "2025-01-30T14:03:44.559659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_json(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ],
   "id": "369ddd1c7e6f42f2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:14.077825Z",
     "start_time": "2025-01-30T14:03:44.578939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Chunking = chunking(directory_path, model_name, chunk_size=100)\n",
    "Embeddings_map = map_document_embeddings(Chunking,AutoTokenizer.from_pretrained(model_name),AutoModel.from_pretrained(model_name) )"
   ],
   "id": "8654fe15915ff8df",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:21.402107Z",
     "start_time": "2025-01-30T14:04:21.397375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_information(query, top_k):\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings=query_embeddings.tolist()\n",
    "    query_embeddings=np.array(query_embeddings)\n",
    "\n",
    "    scores = {}\n",
    "    for doc_id, chunk_dict in Embeddings_map.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "            chunk_embeddings = np.array(chunk_embeddings)\n",
    "\n",
    "            normalized_query = np.linalg.norm(query_embeddings)\n",
    "            normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "\n",
    "            if normalized_chunk == 0 or normalized_query == 0:\n",
    "                score == 0\n",
    "            else:\n",
    "                score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)\n",
    "\n",
    "            scores[(doc_id, chunk_id )] = score\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)\n",
    "\n",
    "    return top_results"
   ],
   "id": "fc4fdb41644ea2f3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:22.108130Z",
     "start_time": "2025-01-30T14:04:22.098023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_json('doc_store.json', Chunking)\n",
    "save_json('vector_store.json', Embeddings_map)\n"
   ],
   "id": "cac16346c94bf09f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:22.502562Z",
     "start_time": "2025-01-30T14:04:22.497240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "document_data= read_json(\"doc_store.json\")\n",
    "vector_data= read_json(\"vector_store.json\")"
   ],
   "id": "8217deba360b7cc8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:23.277929Z",
     "start_time": "2025-01-30T14:04:23.275275Z"
    }
   },
   "cell_type": "code",
   "source": "query = \" What is the title of the paper ?\"",
   "id": "8f863a5edb1ca0eb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:23.791953Z",
     "start_time": "2025-01-30T14:04:23.754996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_match = retrieve_information(query,1)\n",
    "first_match"
   ],
   "id": "38f282131eb0b305",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('193a774a-9f56-40c9-b1fd-80a802cf90b3',\n",
       "  'b87151de-b7e2-45a7-b496-8b8b110903f4',\n",
       "  0.5217984002372201)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:24.269754Z",
     "start_time": "2025-01-30T14:04:24.266240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc_id = first_match[0][0]\n",
    "doc_id"
   ],
   "id": "21f82dfd7bd29d9c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'193a774a-9f56-40c9-b1fd-80a802cf90b3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:24.660158Z",
     "start_time": "2025-01-30T14:04:24.657811Z"
    }
   },
   "cell_type": "code",
   "source": "chunk_id = first_match[0][1]",
   "id": "d700a68f374f7499",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:25.036318Z",
     "start_time": "2025-01-30T14:04:25.033829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "related_text = document_data[doc_id][chunk_id]\n",
    "print(related_text)"
   ],
   "id": "efc3868abbbe2f98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'on\\npharmaceuticals, addressing astronaut safety,\\ndrug discovery acceleration, benefits for Earth,\\nand drug development needs for space missions.\\nInnovations like 3D printing and microfluidics\\nare crucial for maintaining medication efficacy\\nand supporting long-duration missions, ensuring\\nthe success and sustainability of human space\\nexploration.\\nWritten by : Saksham Tiwari and Heramb\\n\\nPaper title: Unraveling the financial dimension of Space endeavors\\nAbstract : Astro-economics, an emerging interdisciplinary field, examines the economic principles\\nand implications of human activities in', 'metadata': {'file_name': 'Abstract'}}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:25.862492Z",
     "start_time": "2025-01-30T14:04:25.860036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ],
   "id": "3101d2ca2847b028",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:27.180572Z",
     "start_time": "2025-01-30T14:04:26.561885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_mistralai import ChatMistralAI  # Changed import\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Mistral model (replaces OpenAI setup)\n",
    "model = ChatMistralAI(\n",
    "    mistral_api_key=os.environ[\"MISTRAL_API_KEY\"],\n",
    "    model=\"mistral-large-latest\"\n",
    ")"
   ],
   "id": "512720d99991859a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:27.889804Z",
     "start_time": "2025-01-30T14:04:27.886365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_lin_response(mistral_model, query, relavent_text):\n",
    "    template = \"\"\"\n",
    "    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n",
    "\n",
    "    Your job is to understand the request, and answer based on the retrieved context.\n",
    "    Here is context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "    chain = prompt | mistral_model\n",
    "\n",
    "    # Fixed parameter casing (Context -> context)\n",
    "    response = chain.invoke({\"context\": relavent_text[\"text\"], \"question\": query})\n",
    "    return response"
   ],
   "id": "47b56cfa68e3bd6e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T14:04:29.797879Z",
     "start_time": "2025-01-30T14:04:28.949305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Usage remains the same\n",
    "generate_lin_response(model, query, related_text)"
   ],
   "id": "19f93b5d8900928d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The title of the paper is \"Unraveling the financial dimension of Space endeavors.\"', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 221, 'total_tokens': 241, 'completion_tokens': 20}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-2647db0a-fee2-4fa6-ad29-3873a134b959-0', usage_metadata={'input_tokens': 221, 'output_tokens': 20, 'total_tokens': 241})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "78808afeb2b48b41",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
